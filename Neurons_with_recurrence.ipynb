{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNklB9uJBXPJhJbdE1Y3Luo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony-gitau/machine_learning_playground/blob/main/Neurons_with_recurrence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am following the [MIT 6.S191 lecture on recurrent neural network](https://youtu.be/ySEx_Bqxvvo) and taking some notes and here I document them.\n",
        "\n",
        "Sequence modelling applications:\n",
        "- machine translation \n",
        "- image captioning\n",
        "- semantic classification\n",
        "\n",
        "\n",
        "Neuron with recurrence\n",
        "- RNN\n",
        "\n",
        "pseudocode of an RNN\n",
        "\n",
        "1. Define the rnn;\n",
        "my_rnn = RNN()\n",
        "2. iterate through all the inputs\n",
        "3. calculate and update the hidden state using an activation function\n",
        "4. generate a predicted output.\n",
        "\n",
        "design criteria for developing networks for sequence modelling:\n",
        "- handle variable lengths\n",
        "- track long dependencies\n",
        "- maintain information about the order of the sequence\n",
        "- share parameters across the sequence\n",
        "\n",
        "example: \n",
        "predicting the next word.\n",
        "\n",
        "1. represent language to a neural network\n",
        "- represent words as numerical representation. \n",
        "\n",
        "one way to represent words as input vectors of a neural network, we use a one hot encoding technique. By one hot encoding we mean, taking a count of every word in a single vector and identifying the word with a 1 and 0 everywhere else. e.g [0,1,0,0] is a one hot vector of a word in the second index (that is appearing second on the count of words in the sequence)\n",
        "\n",
        "2. Training and learning through neural networks\n",
        "\n",
        "- backpropagation through time.\n",
        "\n",
        "challenges:\n",
        "1. exploding gradients\n",
        "- the gradient gets bigger and bigger until its unfeaseble to calculate it, and by extension, training a model becomes unstable.\n",
        "2. vanishing gradients\n",
        "- the gradient on the other hand gets smaller and smaller, until it becomes insignificant.\n",
        "\n",
        "tricks to overcome the challenges:\n",
        "1. changing activation functions\n",
        "e.g ReLU is an a function that prevents the gradient from shrinking\n",
        "2. parameter initialization\n",
        "3. introducing gated cells.\n",
        "select flow of information in the neural network. like the LSTMs\n",
        "\n",
        "applications and limitations of RNN\n",
        "\n",
        "Music generation\n",
        "- Design an RNN that can predict the next musical note.\n",
        "\n",
        "limitation\n",
        "- encoding bottleneck\n",
        "- no easy parallelization techniques\n",
        "- not that long memory for quite long sequences, like the 10,000s of words\n",
        "\n",
        "Attention is all you need:\n",
        "- attend to the most import part of an input example.\n",
        "- extract the features deserve the highest attention.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cpJyUI9IDMQF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuvW5pEkDE0K"
      },
      "outputs": [],
      "source": []
    }
  ]
}